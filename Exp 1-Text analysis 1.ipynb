{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13278c7",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ea6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec92894",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"Hello ! How are you ? How you doing?. I'm going for long ride do you want me to join.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b473754",
   "metadata": {},
   "source": [
    "## Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb54f3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello !', 'How are you ?', 'How you doing?.', \"I'm going for long ride do you want me to join.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf12fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed1283e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello !',\n",
       " 'How are you ?',\n",
       " 'How you doing?.',\n",
       " \"I'm going for long ride do you want me to join.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5e787",
   "metadata": {},
   "source": [
    "## Word Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd18123",
   "metadata": {},
   "source": [
    "###  A) using split method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dd032cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'How',\n",
       " 'you',\n",
       " 'doing?.',\n",
       " \"I'm\",\n",
       " 'going',\n",
       " 'for',\n",
       " 'long',\n",
       " 'ride',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'me',\n",
       " 'to',\n",
       " 'join.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=txt.split(\" \")      #using space as delimeter\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89b8f2",
   "metadata": {},
   "source": [
    "### B) using word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af58bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13265008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'How',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " '.',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'for',\n",
       " 'long',\n",
       " 'ride',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'me',\n",
       " 'to',\n",
       " 'join',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7ddd2",
   "metadata": {},
   "source": [
    "### c)Punctuation based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "584cc660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'How', 'are', 'you', '?', 'How', 'you', 'doing', '?.', 'I', \"'\", 'm', 'going', 'for', 'long', 'ride', 'do', 'you', 'want', 'me', 'to', 'join', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeeff4a",
   "metadata": {},
   "source": [
    "###  d) Tree bank Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ec033b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'How',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " '.',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'for',\n",
       " 'long',\n",
       " 'ride',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'me',\n",
       " 'to',\n",
       " 'join',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce034a39",
   "metadata": {},
   "source": [
    "### E) Tweeet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50f49997",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2=\"HeyðŸ¥° WhaaaatðŸ˜¯ wowðŸ˜®\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f60b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', 'ðŸ¥°', 'Whaaaat', 'ðŸ˜¯', 'wow', 'ðŸ˜®']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer2=TweetTokenizer()\n",
    "tokenizer2.tokenize(sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fbc7b",
   "metadata": {},
   "source": [
    "###  F) MWET tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fee46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt3=\"I was dancing on the floor while drinking.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be95d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'dancing', 'on', 'the', 'floor', 'while', 'drinking', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(txt3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ece15c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b972d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'dancing', 'on_the', 'floor', 'while', 'drinking', '.']\n"
     ]
    }
   ],
   "source": [
    "mwe_tokenizer=MWETokenizer()\n",
    "mwe_tokenizer.add_mwe(('on','the'))\n",
    "\n",
    "print(mwe_tokenizer.tokenize(word_tokenize(txt3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55f2a0",
   "metadata": {},
   "source": [
    "# Stemming and lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7664a093",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2400509",
   "metadata": {},
   "source": [
    "### Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97de76c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easily--->easili\n",
      "fairly--->fairli\n",
      "dancing--->danc\n",
      "playing--->play\n",
      "eating--->eat\n",
      "run--->run\n",
      "running--->run\n",
      "runner--->runner\n",
      "ran--->ran\n",
      "runs--->run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "words=['easily','fairly','dancing','playing','eating','run','running','runner','ran','runs']\n",
    "for word in words:\n",
    "    print(f\"{word}--->{stemmer.stem(word)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac3b14",
   "metadata": {},
   "source": [
    "###  Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "877de726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easily--->easili\n",
      "fairly--->fair\n",
      "dancing--->danc\n",
      "playing--->play\n",
      "eating--->eat\n",
      "run--->run\n",
      "running--->run\n",
      "runner--->runner\n",
      "ran--->ran\n",
      "runs--->run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stemmer=SnowballStemmer(language='english')\n",
    "words=['easily','fairly','dancing','playing','eating','run','running','runner','ran','runs']\n",
    "for word in words:\n",
    "    print(f\"{word}--->{snow_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01979fd",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8626b1",
   "metadata": {},
   "source": [
    "##  Wordnet Lemmatizer technique   which is better than stemming techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b12b2a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe92741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1eb4a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easily\n",
      "fairly\n",
      "dancing\n",
      "playing\n",
      "eating\n",
      "run\n",
      "running\n",
      "runner\n",
      "ran\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmat=WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(lemmat.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27d6b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt3=\"Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87117120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'a',\n",
       " 'little',\n",
       " 'or',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'or',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'in_spite_of']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "mwe_token2= MWETokenizer()\n",
    "mwe_token2.add_mwe(('in', 'spite', 'of'))\n",
    "mwe_token2.tokenize(\"In a little or a little bit or a lot in spite of\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5617c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there',\n",
       " 'is',\n",
       " 'very_small',\n",
       " 'dog',\n",
       " 'on',\n",
       " 'the',\n",
       " 'very',\n",
       " 'huge',\n",
       " 'mountain',\n",
       " 'and',\n",
       " 'there',\n",
       " 'is',\n",
       " 'raining',\n",
       " 'a',\n",
       " 'lot']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mwe_token3=MWETokenizer()\n",
    "mwe_token3.add_mwe(('very','small'))\n",
    "mwe_token3.tokenize(\"there is very small dog on the very huge mountain and there is raining a lot\".split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
